{
  "name": "Expo + Supabase + Ollama (Llama 3.2)",
  "dockerFile": "Dockerfile",

  "features": {
    "ghcr.io/devcontainers/features/node:1": {
      "version": "20"
    },
    "ghcr.io/devcontainers/features/docker-in-docker:2": {},
    "ghcr.io/devcontainers/features/github-cli:1": {}
  },

  // Installation après création du container
  "postCreateCommand": {
    "expo-eas-cli": "npm install -g expo-cli eas-cli",
    "supabase-cli": "npm install -g supabase",
    "ollama": "curl -fsSL https://ollama.com/install.sh | sh && ollama pull llama3.2:8b"
  },

  // Démarrage automatique d’Ollama en arrière-plan (sinon il faut le lancer manuellement)
  "postStartCommand": "ollama serve &",

  // Ports à forward (important pour Expo et Ollama)
  "forwardPorts": [
    11434,     // Ollama API
    19000,     // Expo dev server (web)
    19001,     // Expo packager
    19002      // Expo Metro bundler
  ],

  // Permet à Ollama d’utiliser le GPU si tu es sur Mac M1/M2/M3 ou Linux avec NVIDIA
  "runArgs": [
    "--gpus=all",           // NVIDIA
    "--device=/dev/kvm"     // Android emulator acceleration si besoin plus tard
  ],

  // Extensions VS Code recommandées (installées automatiquement à l’ouverture)
  "customizations": {
    "vscode": {
      "extensions": [
        "supabase.supabase",
        "expo.vscode-expo-tools",
        "prisma.prisma",
        "rangav.vscode-thunder-client",
        "ms-vscode.cpptools",           // utile si tu touches à Ollama plus tard
        "eamodio.gitlens",
        "github.copilot",
        "github.copilot-chat"
      ],
      "settings": {
        "terminal.integrated.defaultProfile.linux": "bash"
      }
    }
  },

  // Pour que nvm fonctionne correctement dans le terminal intégré
  "remoteEnv": {
    "PATH": "${containerEnv:PATH}:/home/vscode/.nvm/versions/node/v20.*/bin"
  },

  // Optionnel : monte ton dossier .ollama local pour garder les modèles entre rebuilds
  "mounts": [
    "source=${localWorkspaceFolderBasename}-ollama,type=volume,target=/root/.ollama"
  ]
}